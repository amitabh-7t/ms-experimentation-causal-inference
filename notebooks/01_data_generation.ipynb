{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b97b841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Dataset generation completed!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------\n",
    "N_USERS = 50000\n",
    "N_DAYS = 30\n",
    "COHORTS = [\"A_control\", \"B_adaptive_v1\", \"C_adaptive_v2\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "# GENERATE USER-LEVEL ATTRIBUTES\n",
    "# ---------------------------------------------\n",
    "user_ids = np.arange(1, N_USERS + 1)\n",
    "\n",
    "users = pd.DataFrame({\n",
    "    \"user_id\": user_ids,\n",
    "    \"region\": np.random.choice([\"US\", \"EU\", \"IN\", \"APAC\"], N_USERS, p=[0.25,0.25,0.30,0.20]),\n",
    "    \"device\": np.random.choice([\"desktop\", \"mobile\"], N_USERS, p=[0.6,0.4]),\n",
    "    \"company_size\": np.random.choice([\"SMB\", \"Mid-Market\", \"Enterprise\"], N_USERS, p=[0.4,0.35,0.25]),\n",
    "    \"baseline_productivity\": np.random.normal(60, 15, N_USERS).clip(5, 100),\n",
    "    \"churn_risk\": np.random.uniform(0,1,N_USERS),\n",
    "    \"power_user\": np.random.binomial(1, 0.15, N_USERS),\n",
    "    \"signup_age\": np.random.randint(5, 400, N_USERS),\n",
    "})\n",
    "\n",
    "# assign cohorts\n",
    "users[\"cohort\"] = np.random.choice(COHORTS, N_USERS, p=[0.34, 0.33, 0.33])\n",
    "users[\"treatment\"] = users[\"cohort\"].apply(lambda x: 0 if x==\"A_control\" else 1)\n",
    "\n",
    "# confounder score (used for causal inference)\n",
    "users[\"confounder_score\"] = (\n",
    "    0.3*users[\"baseline_productivity\"] +\n",
    "    10*users[\"power_user\"] +\n",
    "    -20*users[\"churn_risk\"] +\n",
    "    np.random.normal(0, 5, N_USERS)\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# DAILY-LEVEL DATA\n",
    "# ---------------------------------------------\n",
    "daily_rows = []\n",
    "\n",
    "for _, row in users.iterrows():\n",
    "    uid = row[\"user_id\"]\n",
    "    cohort = row[\"cohort\"]\n",
    "    treatment = row[\"treatment\"]\n",
    "    base_prod = row[\"baseline_productivity\"]\n",
    "    power_user = row[\"power_user\"]\n",
    "    confounder = row[\"confounder_score\"]\n",
    "\n",
    "    for d in range(N_DAYS):\n",
    "        # day index\n",
    "        day = d + 1\n",
    "\n",
    "        # AI usage intensity\n",
    "        ai_calls = np.random.poisson(2 + 0.02*base_prod + 4*treatment + 1.5*power_user)\n",
    "        \n",
    "        # tokens generated â€“ depends on intensity\n",
    "        tokens_generated = int(ai_calls * np.random.normal(300, 50))\n",
    "\n",
    "        # tasks completed via AI\n",
    "        tasks_completed = max(0, int(ai_calls * np.random.uniform(0.2, 0.6)))\n",
    "        \n",
    "        # time spent on platform\n",
    "        time_on_platform = max(1, np.random.normal(20 + 3*treatment + power_user*8, 5))\n",
    "\n",
    "        # satisfaction score\n",
    "        satisfaction_score = np.clip(\n",
    "            np.random.normal(3 + 0.6*treatment + 0.3*power_user, 0.7),\n",
    "            1,5\n",
    "        )\n",
    "\n",
    "        # retention probability increases if productivity or satisfaction is high\n",
    "        retention_probability = (\n",
    "            0.2 +\n",
    "            0.002*base_prod +\n",
    "            0.1*treatment +\n",
    "            0.1*satisfaction_score +\n",
    "            0.2*power_user\n",
    "        ) / 2.5\n",
    "        \n",
    "        retention_7d = np.random.binomial(1, min(max(retention_probability, 0.05), 0.95))\n",
    "\n",
    "        # revenue: influenced by power users + treatment effectiveness\n",
    "        revenue = np.random.choice([0, 10, 20, 50], p=[0.75, 0.15, 0.08, 0.02])\n",
    "        if power_user:\n",
    "            revenue *= 1.5\n",
    "        if treatment:\n",
    "            revenue *= 1.2\n",
    "\n",
    "        daily_rows.append([\n",
    "            uid, day, cohort, treatment, ai_calls, tokens_generated, tasks_completed,\n",
    "            satisfaction_score, retention_7d, revenue, time_on_platform,\n",
    "            row[\"region\"], row[\"device\"], row[\"company_size\"], row[\"signup_age\"],\n",
    "            row[\"confounder_score\"], row[\"power_user\"], row[\"baseline_productivity\"],\n",
    "            row[\"churn_risk\"]\n",
    "        ])\n",
    "\n",
    "daily = pd.DataFrame(daily_rows, columns=[\n",
    "    \"user_id\", \"day\", \"cohort\", \"treatment\", \"ai_calls\", \"tokens_generated\",\n",
    "    \"tasks_completed\", \"satisfaction_score\", \"retention_7d\", \"revenue\",\n",
    "    \"time_on_platform\", \"region\", \"device\", \"company_size\", \"signup_age\",\n",
    "    \"confounder_score\", \"power_user\", \"baseline_productivity\", \"churn_risk\"\n",
    "])\n",
    "\n",
    "# ---------------------------------------------\n",
    "# USER-LEVEL AGGREGATION\n",
    "# ---------------------------------------------\n",
    "user_agg = daily.groupby(\"user_id\").agg({\n",
    "    \"ai_calls\":\"mean\",\n",
    "    \"tokens_generated\":\"mean\",\n",
    "    \"tasks_completed\":\"mean\",\n",
    "    \"satisfaction_score\":\"mean\",\n",
    "    \"time_on_platform\":\"mean\",\n",
    "    \"revenue\":\"sum\",\n",
    "    \"retention_7d\":\"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "user_agg = users.merge(user_agg, on=\"user_id\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# SAVE FILES\n",
    "# ---------------------------------------------\n",
    "# ---------------------------------------------\n",
    "# SAVE FILES\n",
    "# ---------------------------------------------\n",
    "daily.to_csv(\"daily_ai_saas_experiment.csv\", index=False)\n",
    "daily.to_parquet(\"daily_ai_saas_experiment.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "user_agg.to_csv(\"user_ai_saas_experiment.csv\", index=False)\n",
    "user_agg.to_parquet(\"user_ai_saas_experiment.parquet\", engine=\"fastparquet\")\n",
    "\n",
    "print(\"ðŸš€ Dataset generation completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112040d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
